{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiOzwRGcGWYy"
      },
      "source": [
        "# Get Repos from github"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFB0gEEEsITK"
      },
      "source": [
        "Ce script Python a pour but de télécharger automatiquement des fichiers Python à partir de dépôts GitHub listés dans un fichier CSV, et de les stocker dans un dossier spécifié sur Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeiXmvsv8GNC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBCohHTLBLJE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def telecharger_fichier(url, chemin):\n",
        "    chemin = os.path.join('/content/drive/My Drive', chemin)\n",
        "    if chemin.endswith('.py'):\n",
        "        os.makedirs(os.path.dirname(chemin), exist_ok=True)\n",
        "        for attempt in range(5):  # Réessayez jusqu'à 5 fois en cas d'échec\n",
        "            try:\n",
        "                reponse = requests.get(url)\n",
        "                if reponse.status_code == 200:\n",
        "                    with open(chemin, 'wb') as f:\n",
        "                        f.write(reponse.content)\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"Erreur lors du téléchargement du fichier {url}: {reponse.status_code}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors du téléchargement du fichier {url}: {e}\")\n",
        "                time.sleep(5)  # Pause de 5 secondes avant de réessayer\n",
        "\n",
        "def parcourir_dossiers_et_telecharger(repo_url, chemin_base, nom_repo, dossier_global, token):\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    api_url = f\"https://api.github.com/repos/{repo_url}/contents/{chemin_base}\"\n",
        "    for attempt in range(5):  # Réessayez jusqu'à 5 fois en cas d'échec\n",
        "        try:\n",
        "            reponse = requests.get(api_url, headers=headers)\n",
        "            if reponse.status_code == 200:\n",
        "                contenu = reponse.json()\n",
        "                for item in contenu:\n",
        "                    chemin_complet = os.path.join(dossier_global, nom_repo, chemin_base, item['name'])\n",
        "                    if item['type'] == 'file' and item['name'].endswith('.py'):\n",
        "                        telecharger_fichier(item['download_url'], chemin_complet)\n",
        "                    elif item['type'] == 'dir':\n",
        "                        parcourir_dossiers_et_telecharger(repo_url, os.path.join(chemin_base, item['name']), nom_repo, dossier_global, token)\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Erreur lors de l'accès à {api_url}: {reponse.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de l'accès à {api_url}: {e}\")\n",
        "            time.sleep(5)  # Pause de 5 secondes avant de réessayer\n",
        "\n",
        "# Configuration initiale\n",
        "dossier_global = \"/content/drive/My Drive/Dataset/Test/ddos/benin\"\n",
        "os.makedirs(dossier_global, exist_ok=True)\n",
        "\n",
        "token=\"xxxx\"\n",
        "# Lire le fichier Excel contenant les dépôts et les auteurs\n",
        "# Lire le fichier CSV contenant les dépôts et les auteurs\n",
        "df = pd.read_csv('/content/benin_ddos.csv')\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    repo_url = f\"{row['Author']}/{row['Repo']}\"\n",
        "    nom_repo = repo_url.split('/')[-1]\n",
        "    projet_path = os.path.join(dossier_global, nom_repo)\n",
        "\n",
        "    # Vérifier si le projet existe déjà\n",
        "    if os.path.exists(projet_path):\n",
        "        print(f\"Projet {row['Repo']} déjà téléchargé. Passer au suivant.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Projet: {row['Repo']}\")\n",
        "    parcourir_dossiers_et_telecharger(repo_url, '', nom_repo, dossier_global, token)\n",
        "    print(f\"Projet num {index} : son nom est : {row['Repo']}\")\n",
        "    time.sleep(1)  # Pause de 1 seconde entre les requêtes pour éviter de surcharger l'API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphE d'appel"
      ],
      "metadata": {
        "id": "zmYb2sjdcNIp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOY8orrqtZy4"
      },
      "source": [
        "Installation de la bibliotheque pycg qui genre les graphe d'appel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx7LQIe1eYdE"
      },
      "outputs": [],
      "source": [
        "#Installation de la bibliotheque pycg qui genre les graphe d'appel\n",
        "!git clone https://github.com/vitsalis/PyCG.git\n",
        "%cd PyCG\n",
        "!pip install .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NzUYY-buef7H"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "gZOD_Ydz_YI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2j9k_BMsnIi"
      },
      "source": [
        "\n",
        "Ce script est  conçu pour parcourir chaque sous-dossier dans un dossier global sur Google Drive, trouver tous les fichiers Python dans chaque projet, puis exécuter un script Python de la bib pycg (PyCG/__main__.py) sur ces fichiers pour générer des fichiers JSON en sortie (call-graph)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HSFCKwuaM1el"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Chemin vers le dossier contenant tous les projets, situé dans Google Drive\n",
        "dossier_global = '/content/drive/MyDrive/Dataset/Test/keylogger/benin'\n",
        "\n",
        "# Chemin vers le dossier de sortie pour les fichiers JSON, également sur Google Drive\n",
        "dossier_json =\"/content/drive/My Drive/Dataset/Test/keylogger/json_benin\"\n",
        "os.makedirs(dossier_json, exist_ok=True)  # Assurez-vous que le dossier de sortie existe\n",
        "i = 1\n",
        "\n",
        "# Parcourir chaque sous-dossier dans le dossier global\n",
        "for projet in os.listdir(dossier_global):\n",
        "    chemin_projet = os.path.join(dossier_global, projet)\n",
        "\n",
        "    if os.path.isdir(chemin_projet):\n",
        "        # Vérifier si le fichier JSON existe déjà\n",
        "        fichier_json = os.path.join(dossier_json, f'{projet}.json')\n",
        "        if os.path.exists(fichier_json):\n",
        "            print(f\"Le fichier JSON pour {projet} existe déjà, passage au projet suivant.\")\n",
        "            continue\n",
        "\n",
        "        # Construire la commande pour chaque projet\n",
        "        commande = [\n",
        "            'python',\n",
        "            'PyCG/__main__.py',\n",
        "            '--package',\n",
        "            chemin_projet,\n",
        "        ]\n",
        "        # Trouver tous les fichiers Python dans le projet\n",
        "        fichiers_python = subprocess.check_output(['find', chemin_projet, '-type', 'f', '-name', '*.py']).decode().splitlines()\n",
        "        commande.extend(fichiers_python)\n",
        "        commande.extend(['-o', fichier_json])\n",
        "\n",
        "        # Exécuter la commande\n",
        "        print(f'{i} début nom = {projet}')\n",
        "        i += 1\n",
        "        subprocess.run(commande)\n",
        "\n",
        "print(\"Terminé !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y67NlcnStF8j"
      },
      "source": [
        "# Visualisation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRPQtixMJpyX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# visualiser le graphe\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "def visualize_call_graph(data: dict):\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes and edges to the graph\n",
        "    for node, children in data.items():\n",
        "        G.add_node(node, size=40)\n",
        "        for child in children:\n",
        "            G.add_edge(node, child)\n",
        "\n",
        "    # Draw the graph\n",
        "    print(G)\n",
        "    pos = nx.spring_layout(G)  # Layout the nodes using spring layout\n",
        "    plt.figure(figsize=(10, 10))  # Set the figure size\n",
        "    nx.draw(G, pos, with_labels=True, node_size=800, node_color=\"skyblue\", font_size=10, font_weight=\"bold\")  # Draw the graph\n",
        "    plt.title(\"Call Graph\")  # Set the title\n",
        "    plt.show()  # Show the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgEKpa1xJtx9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# script de netoiyage de graphe\n",
        "import json\n",
        "\n",
        "# Fonction pour supprimer les builtin\n",
        "def supprimer_builtin_nodes(data):\n",
        "    if isinstance(data, dict):\n",
        "        keys_to_delete = [key for key in data.keys() if key.startswith(\"<builtin>.\")]\n",
        "        for key in keys_to_delete:\n",
        "            del data[key]\n",
        "        for key, value in data.items():\n",
        "            if isinstance(value, list):\n",
        "                data[key] = [item for item in value if not item.startswith(\"<builtin>.\")]\n",
        "            elif isinstance(value, dict):\n",
        "                data[key] = supprimer_builtin_nodes(value)\n",
        "    elif isinstance(data, list):\n",
        "        data = [item for item in data if not item.startswith(\"<builtin>.\")]\n",
        "    return data\n",
        "\n",
        "\n",
        "# Fonction pour supprimer les nœuds sans référence entrante et sans nœuds voisins\n",
        "def supprimer_noeuds_sans_reference(data, references):\n",
        "    for key, value in list(data.items()):\n",
        "        if key not in references and len(value) == 0:\n",
        "            del data[key]\n",
        "        elif isinstance(value, dict):\n",
        "            supprimer_noeuds_sans_reference(value, references)\n",
        "\n",
        "# Fonction pour obtenir toutes les clés référencées\n",
        "def obtenir_references(data):\n",
        "    references = set()\n",
        "    if isinstance(data, dict):\n",
        "        for value in data.values():\n",
        "            references |= obtenir_references(value)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            if isinstance(item, str):\n",
        "                references.add(item)\n",
        "            elif isinstance(item, dict):\n",
        "                references |= obtenir_references(item)\n",
        "    return references\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoCVct-VJtpL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dossier_json = '/content/drive/MyDrive/json'\n",
        "\n",
        "for fichier_json in os.listdir(dossier_json):\n",
        "    chemin_json = os.path.join(dossier_json, fichier_json)\n",
        "    if os.path.isfile(chemin_json) and fichier_json.endswith('.json'):\n",
        "        # Charger le fichier JSON\n",
        "        with open(chemin_json, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            visualize_call_graph(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7nTDklsTCIF"
      },
      "source": [
        "#  GRAPH (TORCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFXNKsCbSE13",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# installer des packages torch\n",
        "!pip install torch torchvision\n",
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWgEL0nfI4fV"
      },
      "source": [
        "Ce bloc de code est très utile pour charger des données de graphe à partir de fichiers JSON, les convertir en format compatible avec PyTorch, et enfin, visualiser le graphe avec des étiquettes informatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCamPnYvj99m"
      },
      "outputs": [],
      "source": [
        "#Ce bloc de code est très utile pour charger des données de graphe à partir de fichiers JSON, les convertir en format compatible avec PyTorch, et enfin, visualiser le graphe avec des étiquettes informatives\n",
        "import json\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Charger le fichier JSON\n",
        "def load_json(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Construire un graphique PyTorch à partir du fichier JSON\n",
        "def json_to_graph(json_data):\n",
        "    # Créer un mapping de noms de nœuds à indices\n",
        "    node_mapping = {name: i for i, name in enumerate(json_data.keys())}\n",
        "\n",
        "    # Créer les listes d'arêtes source et destination\n",
        "    edge_index = [[], []]\n",
        "    for source, targets in json_data.items():\n",
        "        source_idx = node_mapping[source]\n",
        "        for target in targets:\n",
        "            target_idx = node_mapping[target]\n",
        "            edge_index[0].append(source_idx)  # Source to target\n",
        "            edge_index[1].append(target_idx)  # Target to source (si non dirigé)\n",
        "\n",
        "    # Convertir les listes en tenseurs Torch\n",
        "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "    # Créer un objet Data\n",
        "    data = Data(edge_index=edge_index_tensor)\n",
        "    return data, node_mapping\n",
        "\n",
        "# Fonction pour visualiser le graphe avec les noms des fonctions\n",
        "def visualize_graph(data, node_mapping):\n",
        "    G = nx.DiGraph()  # Utiliser DiGraph pour un graphe dirigé\n",
        "    # Ajouter les arêtes au graphique\n",
        "    edge_index = data.edge_index.numpy()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        src, dest = edge_index[:, i]\n",
        "        G.add_edge(src, dest)\n",
        "\n",
        "    # Dessiner le graphe avec les étiquettes de nœuds\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    pos = nx.spring_layout(G)  # positions for all nodes\n",
        "    nx.draw(G, pos, with_labels=False, node_color='skyblue', node_size=500, edge_color='k', linewidths=1, font_size=15, arrows=True, arrowstyle='-|>', arrowsize=10)\n",
        "\n",
        "    # Dessiner les étiquettes des nœuds avec les noms de fonctions\n",
        "    labels = {i: name for name, i in node_mapping.items()}\n",
        "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
        "\n",
        "    plt.title('Visualisation du graphe avec noms de fonctions')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEmo8f7OHtQe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Chemin vers le fichier JSON\n",
        "json_path = '/content/fireELF.json'  # Modifiez le chemin selon votre environnement\n",
        "\n",
        "# Charger et convertir le JSON en graphique\n",
        "json_data = load_json(json_path)\n",
        "graph_data, node_mapping = json_to_graph(json_data)\n",
        "\n",
        "# Visualiser le graphe\n",
        "visualize_graph(graph_data, node_mapping)\n",
        "\n",
        "print(graph_data, node_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vksZ16A7bCQw"
      },
      "source": [
        "# EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBOJpNg1XuKO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install code-bert-score\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"neulab/codebert-python\")\n",
        "model = AutoModel.from_pretrained(\"neulab/codebert-python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TbaEQXO-pWr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "#Cette fonction prend en entrée du code source, un tokenizer et un modèle de transformer, et retourne un embedding représentant le code. Elle divise le code en segments, les encode avec le modèle et calcule la moyenne des embeddings des segments.\n",
        "def get_embedding(code, tokenizer, model):\n",
        "    segment_size = 512\n",
        "    segments = [code[i:i+segment_size] for i in range(0, len(code), segment_size)]\n",
        "    segment_embeddings = []\n",
        "\n",
        "    for segment in segments:\n",
        "        inputs = tokenizer(segment, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        segment_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()  # Utilisez squeeze() pour éliminer les dimensions de taille 1\n",
        "        segment_embeddings.append(segment_embedding)\n",
        "\n",
        "    if segment_embeddings:\n",
        "        embedding = torch.stack(segment_embeddings).mean(dim=0)  # Moyenne des embeddings\n",
        "    else:\n",
        "        embedding = torch.zeros(model.config.hidden_size)  # Assurez-vous que la dimension est correcte\n",
        "\n",
        "    return embedding\n",
        "\n",
        "\n",
        "#################################################\n",
        "##Cette fonction prend une chaîne de code et transforme  en une importation de module. pour les appel system.\n",
        "def transform_code_chain(code_chain):\n",
        "    # Diviser la chaîne de code en mots\n",
        "    words = code_chain.split('.')\n",
        "\n",
        "    # Si la chaîne ne contient qu'un seul mot, pas de transformation nécessaire\n",
        "    if len(words) == 1:\n",
        "        return code_chain\n",
        "\n",
        "    # Extraire le premier mot comme nom du module à importer\n",
        "    module_to_import = words[0]\n",
        "\n",
        "    # Reconstituer la chaîne de code avec importation du premier mot\n",
        "    transformed_code = f\"import {module_to_import}\\n{code_chain}\"\n",
        "\n",
        "    return transformed_code\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################3\n",
        "\n",
        "# Cette fonction extrait le code source d'une fonction à partir d'un fichier Python donné et du nom de la fonction.\n",
        "import ast\n",
        "\n",
        "def get_function_code(filename, function_name):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            file_content = file.read()\n",
        "            tree = ast.parse(file_content)\n",
        "            for node in ast.walk(tree):\n",
        "                if isinstance(node, ast.FunctionDef) and node.name == function_name:\n",
        "                    return ast.unparse(node)  # Fonctionne avec Python 3.9+\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Le fichier {filename} n'a pas été trouvé.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'analyse du fichier : {e}\")\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#Cette fonction prend un chemin de répertoire en entrée et retourne une liste de chemins de fichiers présents dans ce répertoire, y compris les sous-répertoires.\n",
        "def lister_chemins_repertoire(OneRepoGitHub):\n",
        "    chemins_fichiers = []\n",
        "    for dossier_parent, dossiers, fichiers in os.walk(OneRepoGitHub):\n",
        "        for fichier in fichiers:\n",
        "\n",
        "            chemin_complet = os.path.join(dossier_parent, fichier)\n",
        "            chemins_fichiers.append(chemin_complet)\n",
        "          #  print(\"lister chemin\", chemins_fichiers)\n",
        "    return chemins_fichiers\n",
        "################################################################################\n",
        "#Cette fonction génère les chemins de fichiers pour les fonctions extraites à partir d'un graphe.\n",
        "def generate_file_paths(RepoGitHub,graph, json_file, path_fichiers):\n",
        "    file_paths = {}\n",
        "    json_file_name = os.path.basename(json_file).split('.')[0]  # Obtenir le nom du fichier JSON sans extension\n",
        "    print(\"json_file_name\", json_file_name)\n",
        "\n",
        "    # Normalisation des chemins dans path_fichiers\n",
        "    for i, path in enumerate(path_fichiers):\n",
        "        temp = '/'.join(path.split('/')[1:])\n",
        "        temp = f'/{temp}'\n",
        "        path_fichiers[i] = temp\n",
        "\n",
        "    # Génération des chemins de fichiers avec informations supplémentaires\n",
        "    for function, _ in graph.items():\n",
        "        parts = function.split('.')\n",
        "        file_path = '/'.join(parts[:-1]) + '.py'\n",
        "        path = os.path.join(RepoGitHub, json_file_name, file_path)\n",
        "\n",
        "        if path in path_fichiers:\n",
        "\n",
        "            file_paths[parts[-1]] = {\"full_path\": f\"{json_file_name}/{file_path}\", \"full_function_name\": function}\n",
        "            #print(f\"cas1 path :{json_file_name}/{file_path}\")\n",
        "        elif os.path.join(RepoGitHub, json_file_name, function.replace('.', '/') + \".py\") in path_fichiers:\n",
        "            temp = function.replace('.', '/') + \".py\"\n",
        "            file_paths[parts[-1]] = {\"full_path\": f'{RepoGitHub}/{json_file_name}/{temp}', \"full_function_name\": function}\n",
        "            #print(f\"cas2 path :{RepoGitHub}/{json_file_name}/{temp}\")\n",
        "\n",
        "        else:\n",
        "            file_paths[parts[-1]] = {\"full_path\": '.'.join(parts), \"full_function_name\": function}\n",
        "            #print(\"cas3 path : \",json_file_name,'.'.join(parts))\n",
        "\n",
        "    return file_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFO_tnZ0E_xZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "#Cette fonction prend en entrée du code source, un tokenizer et un modèle de transformer, et retourne un embedding représentant le code. Elle divise le code en segments, les encode avec le modèle et calcule la moyenne des embeddings des segments.\n",
        "def get_embedding(code, tokenizer, model):\n",
        "    segment_size = 512\n",
        "    segments = [code[i:i+segment_size] for i in range(0, len(code), segment_size)]\n",
        "    segment_embeddings = []\n",
        "\n",
        "    for segment in segments:\n",
        "        inputs = tokenizer(segment, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        segment_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()  # Utilisez squeeze() pour éliminer les dimensions de taille 1\n",
        "        segment_embeddings.append(segment_embedding)\n",
        "\n",
        "    if segment_embeddings:\n",
        "        embedding = torch.stack(segment_embeddings).mean(dim=0)  # Moyenne des embeddings\n",
        "    else:\n",
        "        embedding = torch.zeros(model.config.hidden_size)  # Assurez-vous que la dimension est correcte\n",
        "\n",
        "    return embedding\n",
        "\n",
        "\n",
        "#################################################\n",
        "##Cette fonction prend une chaîne de code et transforme  en une importation de module. pour les appel system.\n",
        "def transform_code_chain(code_chain):\n",
        "    # Diviser la chaîne de code en mots\n",
        "    words = code_chain.split('.')\n",
        "\n",
        "    # Si la chaîne ne contient qu'un seul mot, pas de transformation nécessaire\n",
        "    if len(words) == 1:\n",
        "        return code_chain\n",
        "\n",
        "    # Extraire le premier mot comme nom du module à importer\n",
        "    module_to_import = words[0]\n",
        "\n",
        "    # Reconstituer la chaîne de code avec importation du premier mot\n",
        "    transformed_code = f\"import {module_to_import}\\n{code_chain}\"\n",
        "\n",
        "    return transformed_code\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################3\n",
        "\n",
        "# Cette fonction extrait le code source d'une fonction à partir d'un fichier Python donné et du nom de la fonction.\n",
        "import ast\n",
        "\n",
        "def get_function_code(filename, function_name):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            file_content = file.read()\n",
        "            tree = ast.parse(file_content)\n",
        "            for node in ast.walk(tree):\n",
        "                if isinstance(node, ast.FunctionDef) and node.name == function_name:\n",
        "                    return ast.unparse(node)  # Fonctionne avec Python 3.9+\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Le fichier {filename} n'a pas été trouvé.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'analyse du fichier : {e}\")\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#Cette fonction prend un chemin de répertoire en entrée et retourne une liste de chemins de fichiers présents dans ce répertoire, y compris les sous-répertoires.\n",
        "def lister_chemins_repertoire(OneRepoGitHub):\n",
        "    chemins_fichiers = []\n",
        "    for dossier_parent, dossiers, fichiers in os.walk(OneRepoGitHub):\n",
        "        for fichier in fichiers:\n",
        "\n",
        "            chemin_complet = os.path.join(dossier_parent, fichier)\n",
        "            chemins_fichiers.append(chemin_complet)\n",
        "          #  print(\"lister chemin\", chemins_fichiers)\n",
        "    return chemins_fichiers\n",
        "################################################################################\n",
        "#Cette fonction génère les chemins de fichiers pour les fonctions extraites à partir d'un graphe.\n",
        "def generate_file_paths(RepoGitHub,graph, json_file, path_fichiers):\n",
        "    file_paths = {}\n",
        "    json_file_name = os.path.basename(json_file).split('.')[0]  # Obtenir le nom du fichier JSON sans extension\n",
        "    print(\"json_file_name\", json_file_name)\n",
        "\n",
        "    # Normalisation des chemins dans path_fichiers\n",
        "    for i, path in enumerate(path_fichiers):\n",
        "        temp = '/'.join(path.split('/')[1:])\n",
        "        temp = f'/{temp}'\n",
        "        path_fichiers[i] = temp\n",
        "\n",
        "    # Génération des chemins de fichiers avec informations supplémentaires\n",
        "    for function, _ in graph.items():\n",
        "        parts = function.split('.')\n",
        "        file_path = '/'.join(parts[:-1]) + '.py'\n",
        "        path = os.path.join(RepoGitHub, json_file_name, file_path)\n",
        "\n",
        "        if path in path_fichiers:\n",
        "\n",
        "            file_paths[parts[-1]] = {\"full_path\": f\"{json_file_name}/{file_path}\", \"full_function_name\": function}\n",
        "            #print(f\"cas1 path :{json_file_name}/{file_path}\")\n",
        "        elif os.path.join(RepoGitHub, json_file_name, function.replace('.', '/') + \".py\") in path_fichiers:\n",
        "            temp = function.replace('.', '/') + \".py\"\n",
        "            file_paths[parts[-1]] = {\"full_path\": f'{RepoGitHub}/{json_file_name}/{temp}', \"full_function_name\": function}\n",
        "            #print(f\"cas2 path :{RepoGitHub}/{json_file_name}/{temp}\")\n",
        "\n",
        "        else:\n",
        "            file_paths[parts[-1]] = {\"full_path\": '.'.join(parts), \"full_function_name\": function}\n",
        "            #print(\"cas3 path : \",json_file_name,'.'.join(parts))\n",
        "\n",
        "    return file_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7vS6QXBAs5g"
      },
      "outputs": [],
      "source": [
        "#ce bloc de code est conçu pour traiter et d'extraire des informations utiles à partir du code source Python, notamment la génération de embeddings pour les fonctions\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"neulab/codebert-python\")\n",
        "model = AutoModel.from_pretrained(\"neulab/codebert-python\")\n",
        "\n",
        "def generate_code_from_json(json_file, RepoGitHub):\n",
        "    repo_name, extension = os.path.splitext(os.path.basename(json_file))\n",
        "    dossier_code = os.path.join('code', repo_name)\n",
        "    os.makedirs(dossier_code, exist_ok=True)\n",
        "    function_embeddings = {}\n",
        "\n",
        "\n",
        "    with open(json_file, \"r\") as file:\n",
        "        graph = json.load(file)\n",
        "\n",
        "    chemins = lister_chemins_repertoire(os.path.join(RepoGitHub, repo_name))\n",
        "    print(\"chemins\", chemins)\n",
        "\n",
        "    # Générer les chemins de fichiers pour chaque fonction\n",
        "    file_paths = generate_file_paths(RepoGitHub,graph, json_file, chemins)\n",
        "\n",
        "    # Traiter chaque fonction\n",
        "    for func_name, details in file_paths.items():\n",
        "        full_path = os.path.join(RepoGitHub, details['full_path'])\n",
        "        path = os.path.join(dossier_code, f\"{func_name}.py\")  # Chemin pour écrire le code\n",
        "\n",
        "        if full_path in chemins:\n",
        "\n",
        "            function_code = get_function_code(full_path, func_name)\n",
        "        else:\n",
        "            transformed_code = transform_code_chain(details['full_path'])\n",
        "            function_code = transformed_code\n",
        "\n",
        "        if function_code:\n",
        "            embedding = get_embedding(function_code,tokenizer, model)\n",
        "            if embedding.requires_grad:\n",
        "                embedding = embedding.detach()  # Détacher le tenseur avant de le convertir en numpy\n",
        "            function_embeddings[details['full_function_name']] = embedding.numpy().tolist()\n",
        "\n",
        "            with open(path, 'w') as file:\n",
        "                file.write(function_code)\n",
        "\n",
        "        else:\n",
        "            if full_path in chemins:\n",
        "                shutil.copyfile(full_path, path)  # Copie du fichier original\n",
        "                # Lecture du fichier pour générer l'embedding\n",
        "                with open(path, 'r') as file:\n",
        "                    entire_code = file.read()\n",
        "                embedding = get_embedding(entire_code,tokenizer, model)\n",
        "                if embedding.requires_grad:\n",
        "                    embedding = embedding.detach()\n",
        "                function_embeddings[details['full_function_name']] = embedding.numpy().tolist()\n",
        "\n",
        "    return function_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH8tP4y1DjmV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Charger le tokenizer et le modèle pré-entraîné CodeBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"neulab/codebert-python\")\n",
        "model = AutoModel.from_pretrained(\"neulab/codebert-python\")\n",
        "\n",
        "def generate_code_from_json(json_file, RepoGitHub):\n",
        "    repo_name, extension = os.path.splitext(os.path.basename(json_file))\n",
        "    dossier_code = os.path.join('code', repo_name)\n",
        "    os.makedirs(dossier_code, exist_ok=True)\n",
        "    function_embeddings = {}\n",
        "\n",
        "    with open(json_file, \"r\") as file:\n",
        "        graph = json.load(file)\n",
        "\n",
        "    chemins = lister_chemins_repertoire(os.path.join(RepoGitHub, repo_name))\n",
        "    print(\"chemins\", chemins)\n",
        "\n",
        "    # Générer les chemins de fichiers pour chaque fonction\n",
        "    file_paths = generate_file_paths(RepoGitHub, graph, json_file, chemins)\n",
        "\n",
        "    # Traiter chaque fonction\n",
        "    for func_name, details in file_paths.items():\n",
        "        full_path = os.path.join(RepoGitHub, details['full_path'])\n",
        "\n",
        "        if full_path in chemins:\n",
        "            function_code = get_function_code(full_path, func_name)\n",
        "        else:\n",
        "            transformed_code = transform_code_chain(details['full_path'])\n",
        "            function_code = transformed_code\n",
        "\n",
        "        if function_code:\n",
        "            embedding = get_embedding(function_code, tokenizer, model)\n",
        "            if embedding.requires_grad:\n",
        "                embedding = embedding.detach()  # Détacher le tenseur avant de le convertir en numpy\n",
        "            function_embeddings[details['full_function_name']] = embedding.numpy().tolist()\n",
        "\n",
        "        else:\n",
        "            if full_path in chemins:\n",
        "                # Lecture du fichier pour générer l'embedding\n",
        "                with open(full_path, 'r') as file:\n",
        "                    entire_code = file.read()\n",
        "                embedding = get_embedding(entire_code, tokenizer, model)\n",
        "                if embedding.requires_grad:\n",
        "                    embedding = embedding.detach()\n",
        "                function_embeddings[details['full_function_name']] = embedding.numpy().tolist()\n",
        "\n",
        "    return function_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIAsF7jdLqJ-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Charger le fichier JSON\n",
        "def load_json(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def json_to_graph(json_data, embeddings):\n",
        "    # Créer un mapping de noms de nœuds à indices\n",
        "    node_mapping = {name: i for i, name in enumerate(json_data.keys())}\n",
        "\n",
        "    # Créer les listes d'arêtes source et destination\n",
        "    edge_index = [[], []]\n",
        "    for source, targets in json_data.items():\n",
        "        source_idx = node_mapping[source]\n",
        "        for target in targets:\n",
        "            target_idx = node_mapping[target]\n",
        "            edge_index[0].append(source_idx)\n",
        "            edge_index[1].append(target_idx)\n",
        "\n",
        "    # Convertir les listes en tenseurs Torch\n",
        "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "    # Associer les embeddings aux nœuds\n",
        "    num_nodes = len(node_mapping)\n",
        "    # Assumer que tous les vecteurs ont la même taille et que tous les noeuds ont un embedding\n",
        "    node_features = torch.zeros((num_nodes, len(next(iter(embeddings.values())))))\n",
        "    for node, idx in node_mapping.items():\n",
        "        if node in embeddings:\n",
        "            # Assurer que l'embedding est un tensor et a la bonne forme\n",
        "            embedding_tensor = torch.tensor(embeddings[node])\n",
        "            if embedding_tensor.ndim == 1:\n",
        "                node_features[idx] = embedding_tensor\n",
        "            else:\n",
        "                raise ValueError(\"Each embedding should be a single-dimensional tensor\")\n",
        "\n",
        "    # Créer un objet Data avec les embeddings\n",
        "    data = Data(edge_index=edge_index_tensor, x=node_features)\n",
        "    return data, node_mapping\n",
        "\n",
        "# Fonction pour visualiser le graphe avec les noms des fonctions\n",
        "def visualize_graph(data, node_mapping):\n",
        "    G = nx.DiGraph()  # Utiliser DiGraph pour un graphe dirigé\n",
        "    # Ajouter les arêtes au graphique\n",
        "    edge_index = data.edge_index.numpy()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        src, dest = edge_index[:, i]\n",
        "        G.add_edge(src, dest)\n",
        "\n",
        "    # Dessiner le graphe avec les étiquettes de nœuds\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    pos = nx.spring_layout(G)  # positions for all nodes\n",
        "    nx.draw(G, pos, with_labels=False, node_color='skyblue', node_size=500, edge_color='k', linewidths=1, font_size=15, arrows=True, arrowstyle='-|>', arrowsize=10)\n",
        "\n",
        "    # Dessiner les étiquettes des nœuds avec les noms de fonctions\n",
        "    labels = {i: name for name, i in node_mapping.items()}\n",
        "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
        "\n",
        "    plt.title('Visualisation du graphe avec noms de fonctions')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCYJS2uAa5Z9"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v42pNeMkQolJ"
      },
      "outputs": [],
      "source": [
        "def get_embedding(code, tokenizer, model):\n",
        "    segment_size = 512\n",
        "    segments = [code[i:i + segment_size] for i in range(0, len(code), segment_size)]\n",
        "    segment_embeddings = []\n",
        "\n",
        "    for segment in segments:\n",
        "        inputs = tokenizer(segment, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        segment_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "        segment_embeddings.append(segment_embedding)\n",
        "\n",
        "    if segment_embeddings:\n",
        "        embedding = torch.stack(segment_embeddings).mean(dim=0)\n",
        "    else:\n",
        "        embedding = torch.zeros(model.config.hidden_size)  # Taille fixe\n",
        "\n",
        "    return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYA8-kzkmfFc"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_graph(data, node_mapping):\n",
        "    G = nx.DiGraph()  # Directed graph\n",
        "    edge_index = data.edge_index.numpy()\n",
        "\n",
        "    # Adding edges to the graph\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        src, dest = edge_index[:, i]\n",
        "        G.add_edge(src, dest)\n",
        "\n",
        "    # Check which nodes are in the graph and node_mapping\n",
        "    graph_nodes = list(G.nodes)\n",
        "    mapped_nodes = list(node_mapping.values())\n",
        "    missing_nodes = set(graph_nodes) - set(mapped_nodes)\n",
        "\n",
        "    # Filter node_mapping to only include nodes that are present\n",
        "    filtered_node_mapping = {i: name for name, i in node_mapping.items() if i in graph_nodes}\n",
        "\n",
        "    # Create a layout for the graph visualization\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    # Draw graph\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    nx.draw(G, pos, with_labels=False, node_color='skyblue', node_size=500, edge_color='k', linewidths=1, font_size=15, arrows=True, arrowstyle='-|>', arrowsize=10)\n",
        "\n",
        "    # Add labels to the nodes based on the function names\n",
        "    nx.draw_networkx_labels(G, pos, filtered_node_mapping, font_size=8)\n",
        "\n",
        "    plt.title('Visualisation du graphe avec noms de fonctions')\n",
        "    plt.show()\n",
        "\n",
        "    if missing_nodes:\n",
        "        print(f\"Warning: Nodes not found in mapping: {missing_nodes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcoAppCZ8W_a"
      },
      "outputs": [],
      "source": [
        "class GraphDataset:\n",
        "    def __init__(self):\n",
        "        self.graphs = []\n",
        "\n",
        "    def add_graph(self, graph_data, node_mapping, label, name ):\n",
        "        self.graphs.append({'data': graph_data, 'mapping': node_mapping, 'label': label,'name':name})\n",
        "\n",
        "    def get_graphs(self):\n",
        "        return self.graphs\n",
        "    def get_filenames(self):\n",
        "        return [graph['name'] for graph in self.graphs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au7zz6MP7175"
      },
      "outputs": [],
      "source": [
        " def get_filenames(self):\n",
        "        return [graph['name'] for graph in self.graphs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcD03ZmbKQlg"
      },
      "outputs": [],
      "source": [
        "# Créer une instance de GraphDataset\n",
        "dataset = GraphDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "329kiboUa_ga"
      },
      "outputs": [],
      "source": [
        "# Créer une instance de GraphDataset\n",
        "dataset = GraphDataset()\n",
        "json_directory='/content/drive/MyDrive/Benin_total_json'\n",
        "myRepoGitHub ='/content/drive/My Drive/Benin_total'\n",
        "\n",
        "\n",
        "# Parcourir tous les fichiers JSON dans le répertoire\n",
        "for json_filename in os.listdir(json_directory):\n",
        "    if json_filename.endswith('.json'):\n",
        "        json_path = os.path.join(json_directory, json_filename)\n",
        "\n",
        "        # Générer le code à partir du fichier JSON\n",
        "        vecteur = generate_code_from_json(json_path, myRepoGitHub)\n",
        "\n",
        "        # Charger les données JSON\n",
        "        json_data = load_json(json_path)\n",
        "\n",
        "        # Convertir les données JSON en graphique\n",
        "        graph_data, node_mapping = json_to_graph(json_data, vecteur)\n",
        "\n",
        "\n",
        "        # Classify the graph and assign a label\n",
        "        label =1\n",
        "\n",
        "        # Ajouter le graphique à l'ensemble de données\n",
        "        dataset.add_graph(graph_data, node_mapping,label,json_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "-r8V3kAS4J22"
      },
      "outputs": [],
      "source": [
        "# Créer une instance de GraphDataset\n",
        "\n",
        "json_directory='/content/drive/MyDrive/Benin_total_json'\n",
        "xRepoGitHub ='/content/drive/MyDrive/Benin_total'\n",
        "import os\n",
        "i = 1\n",
        "# Parcourir tous les fichiers JSON dans le répertoire\n",
        "for json_filename in os.listdir(json_directory):\n",
        "\n",
        "    if json_filename.endswith('.json'):\n",
        "        print(f'fichier nume : {i} : nom : {json_filename}')\n",
        "        i = i+1\n",
        "        json_path = os.path.join(json_directory, json_filename)\n",
        "\n",
        "        # Générer le code à partir du fichier JSON\n",
        "        ft = generate_code_from_json(json_path, xRepoGitHub)\n",
        "\n",
        "        # Charger les données JSON\n",
        "        json_data = load_json(json_path)\n",
        "\n",
        "        # Convertir les données JSON en graphique\n",
        "        graph_data, node_mapping = json_to_graph(json_data, ft)\n",
        "\n",
        "\n",
        "        # Classify the graph and assign a label\n",
        "        label =0\n",
        "\n",
        "        # Ajouter le graphique à l'ensemble de données\n",
        "        dataset.add_graph(graph_data, node_mapping,label,json_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nNNjYT_y7NtI"
      },
      "outputs": [],
      "source": [
        "# Créer un ensemble pour stocker les noms de fichiers déjà ajoutés\n",
        "existing_files = set(dataset.get_filenames())\n",
        "\n",
        "json_directory='/content/drive/MyDrive/Benin_total_json'\n",
        "xRepoGitHub ='/content/drive/MyDrive/Benin_total'\n",
        "import os\n",
        "i = 1\n",
        "# Parcourir tous les fichiers JSON dans le répertoire\n",
        "for json_filename in os.listdir(json_directory):\n",
        "\n",
        "    if json_filename.endswith('.json'):\n",
        "        if json_filename in existing_files:\n",
        "            print(f'Le fichier {json_filename} existe déjà dans le dataset. Ne pas ajouter une deuxième fois.')\n",
        "            continue\n",
        "\n",
        "        print(f'fichier nume : {i} : nom : {json_filename}')\n",
        "        i = i+1\n",
        "        json_path = os.path.join(json_directory, json_filename)\n",
        "\n",
        "        # Générer le code à partir du fichier JSON\n",
        "        ft = generate_code_from_json(json_path, xRepoGitHub)\n",
        "\n",
        "        # Charger les données JSON\n",
        "        json_data = load_json(json_path)\n",
        "\n",
        "        # Convertir les données JSON en graphique\n",
        "        graph_data, node_mapping = json_to_graph(json_data, ft)\n",
        "\n",
        "        # Classify the graph and assign a label\n",
        "        label = 0\n",
        "\n",
        "        # Ajouter le graphique à l'ensemble de données\n",
        "        dataset.add_graph(graph_data, node_mapping, label, json_filename)\n",
        "\n",
        "        # Mettre à jour l'ensemble des fichiers existants\n",
        "        existing_files.add(json_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sf_0BJqfCD7O"
      },
      "outputs": [],
      "source": [
        "labeled_graphs = dataset.get_graphs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cybg4kWNSPEa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Suppose que labeled_graphs contient vos graphes\n",
        "#labeled_graphs = dataset.get_graphs()\n",
        "\n",
        "# Enregistrement des graphes dans un fichier\n",
        "with open('/content/drive/MyDrive/Dataset/labeled_graphs.pkl', 'wb') as f:\n",
        "    pickle.dump(labeled_graphs, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC8iOgoiSR5r"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/labeled_graphs.pkl', 'rb') as f:\n",
        "    mon_dataset= pickle.load(f)\n",
        "print(len(mon_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y4Rb9EWCgqbY"
      },
      "outputs": [],
      "source": [
        "# Visualiser tous les graphes dans l'ensemble de données\n",
        "for i, graph_entry in enumerate(mon_dataset, start=1):\n",
        "    graph_data = graph_entry['data']\n",
        "    node_mapping = graph_entry['mapping']\n",
        "    name =  graph_entry['name']\n",
        "    label = graph_entry['label']\n",
        "    print(f\"Graphe {i}:\")\n",
        "    print(\"Données du graphe:\")\n",
        "    print(graph_data)\n",
        "    print(\"Mapping des nœuds:\")\n",
        "    print(node_mapping)\n",
        "    print(f\"  Label: {label}\")\n",
        "    print(f\"  Name:   {name}\")\n",
        "    visualize_graph(graph_data, node_mapping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4WSu_XDXSYEf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualiser tous les graphes dans l'ensemble de données\n",
        "for i, graph_entry in enumerate(x, start=1):\n",
        "    graph_data = graph_entry['data']\n",
        "    node_mapping = graph_entry['mapping']\n",
        "    label = graph_entry['label']\n",
        "    name = graph_entry['name']\n",
        "\n",
        "    print(f\"\\nGraphe {i}:\")\n",
        "    print(\"Données du graphe:\")\n",
        "    print(graph_data)\n",
        "\n",
        "    # Récupérer les features des nœuds\n",
        "    node_features = graph_data.x\n",
        "    num_nodes = node_features.size(0)  # Taille: nombre de nœuds\n",
        "    feature_dim = node_features.size(1)  # Dimension des features\n",
        "\n",
        "    print(f\"  Nombre de nœuds: {num_nodes}\")\n",
        "    print(f\"  Dimension des features des nœuds: {feature_dim}\")\n",
        "\n",
        "    # Afficher les features pour chaque nœud\n",
        "    for node_index, features in enumerate(node_features):\n",
        "        print(f\"    Nœud {node_index}: Features = {features.tolist()}\")\n",
        "\n",
        "    print(\"Mapping des nœuds:\")\n",
        "    print(node_mapping)\n",
        "    print(f\"  Label: {label}\")\n",
        "    print(f\"  Name : {name}\")\n",
        "\n",
        "    # Visualiser le graphe\n",
        "    visualize_graph(graph_data, node_mapping)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VM2q48-G8Sr"
      },
      "source": [
        "#  GNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/labeled_graphs.pkl', 'rb') as f:\n",
        "    mon_dataset= pickle.load(f)"
      ],
      "metadata": {
        "id": "kh9695PVp2r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lYRci7KW8FL"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "pytorch_geometric_graphs = [\n",
        "    Data(x=entry['data'].x, edge_index=entry['data'].edge_index, y=torch.tensor([entry['label']]) , name=entry['name'])\n",
        "    for entry in mon_dataset\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR2EJOmENA7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.loader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kiR1m5j_W-hP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Divisez le dataset en ensembles d'entraînement et de test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "train_graphs, test_graphs = train_test_split(pytorch_geometric_graphs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Créez les loaders d'entraînement et de test\n",
        "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlz9BV96GstJ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "        self.bn3 = BatchNorm(output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO1jHbgxXFby"
      },
      "outputs": [],
      "source": [
        "# Initialisation du Modèle, Optimiseur, et Critère de Perte\n",
        "input_dim = 768  # Ajustez selon vos données\n",
        "hidden_dim = 384\n",
        "output_dim = 64 # Ajustez pour correspondre aux classes\n",
        "\n",
        "modelGNN = GCNWithNorm(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(modelGNN.parameters(), lr=0.0001, weight_decay=5e-4)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXtoZunjXJA7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fonction d'entraînement\n",
        "def train(modeltrain):\n",
        "    modeltrain.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = modeltrain(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "# Fonction de Test\n",
        "# Fonction de Test\n",
        "def test(loader, modeltest):\n",
        "    modeltest.eval()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            out = modeltest(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item() * data.num_graphs  # Multiplication par le nombre de graphes dans le batch\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            total_samples += data.num_graphs\n",
        "    accuracy = correct / total_samples\n",
        "    average_loss = total_loss / total_samples\n",
        "    return accuracy, average_loss  # Retourner à la fois l'exactitude et la perte moyenne\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwT1bQOty9jg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Boucle d'entraînement avec visualisation\n",
        "epochs = 500\n",
        "train_losses = []\n",
        "test_losses = []  # Ajout d'une liste pour stocker les pertes de test\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(modelGNN)\n",
        "    train_acc, _ = test(train_loader, modelGNN)  # Utilisation de la fonction de test pour obtenir l'exactitude\n",
        "    test_acc, test_loss = test(test_loader, modelGNN)  # Utilisation de la fonction de test pour obtenir l'exactitude et la perte\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)  # Stockage de la perte de test\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f'Epoch {epoch:02d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, epochs + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, epochs + 1), test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(modelGNN)"
      ],
      "metadata": {
        "id": "4troAFOjqfDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GjgMVNEXeeBK"
      },
      "outputs": [],
      "source": [
        "# Le modèle GNN Simplifié\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Fonction pour afficher les embeddings après chaque époque\n",
        "def display_graph_embeddings(loader, epoch , modeldisplay):\n",
        "    modeldisplay.eval()\n",
        "    graph_embeddings = []\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            embeddings = modeldisplay.forward(data)\n",
        "            graph_embeddings.append((data.name , embeddings))\n",
        "\n",
        "    # Afficher les embeddings\n",
        "    print(f\"\\nEmbeddings des graphes à l'époque {epoch}:\")\n",
        "    for name, embedding in graph_embeddings:\n",
        "        print(f\"Graphe {name}: Embedding = {embedding.numpy()}\")\n",
        "\n",
        "# Boucle d'entraînement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg0EtTPb0Yqr"
      },
      "outputs": [],
      "source": [
        "# Accéder et afficher un graphe par son index\n",
        "def display_graph_by_index(dataset, index):\n",
        "    # Vérifiez que l'index est valide\n",
        "    if index < 0 or index >= len(dataset.get_graphs()):\n",
        "        print(f\"Index {index} hors limites.\")\n",
        "        return\n",
        "\n",
        "    # Récupérer le graphe en utilisant l'index\n",
        "    graph_entry = dataset.get_graphs()[index]\n",
        "    graph_data = graph_entry['data']\n",
        "    node_mapping = graph_entry['mapping']\n",
        "    label = graph_entry['label']\n",
        "    name = graph_entry.get('name', f'Graph {index}')\n",
        "\n",
        "    # Afficher les informations du graphe\n",
        "    print(f\"\\nGraphe {index} ({name}):\")\n",
        "    print(\"Données du graphe:\")\n",
        "    print(graph_data)\n",
        "\n",
        "    # Récupérer les features des nœuds\n",
        "    node_features = graph_data.x\n",
        "    num_nodes = node_features.size(0)  # Taille: nombre de nœuds\n",
        "    feature_dim = node_features.size(1)  # Dimension des features\n",
        "\n",
        "    print(f\"  Nombre de nœuds: {num_nodes}\")\n",
        "    print(f\"  Dimension des features des nœuds: {feature_dim}\")\n",
        "\n",
        "    # Afficher les features pour chaque nœud\n",
        "    for node_index, features in enumerate(node_features):\n",
        "        print(f\"    Nœud {node_index}: Features = {features.tolist()}\")\n",
        "\n",
        "    print(\"Mapping des nœuds:\")\n",
        "    print(node_mapping)\n",
        "    print(f\"  Label: {label}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HA2FWe-l01n7"
      },
      "outputs": [],
      "source": [
        "display_graph_by_index(dataset , 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SWkF6DsK1MCI"
      },
      "outputs": [],
      "source": [
        "def get_all_embeddings(model, loader):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_names = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Itérer sur chaque batch dans le DataLoader\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # Obtenir les embeddings du modèle pour le batch actuel\n",
        "            embeddings = model.forward(data)\n",
        "\n",
        "        # Stocker les embeddings\n",
        "        all_embeddings.extend(embeddings.cpu().numpy())\n",
        "\n",
        "        # Si les noms sont stockés dans le DataLoader, les récupérer\n",
        "        if hasattr(data, 'name'):\n",
        "            all_names.extend(data.name)\n",
        "\n",
        "\n",
        "        if hasattr(data, 'y'):\n",
        "            all_labels.extend(data.y)\n",
        "\n",
        "\n",
        "    return all_embeddings, all_names ,  all_labels\n",
        "\n",
        "# Utilisation de la fonction\n",
        "loader_test = DataLoader(pytorch_geometric_graphs, batch_size=2, shuffle=True)\n",
        "embeddings, names , labels = get_all_embeddings(modelGNN, loader_test)\n",
        "\n",
        "# Afficher les résultats pour vérifier\n",
        "for name, embedding ,label   in zip(names, embeddings, labels):\n",
        "    print(f\"Nom du Graphe: {name}, Label du Graphe: {label}, Embedding: {embedding}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAuHv-g35oiZ"
      },
      "outputs": [],
      "source": [
        "# Chargement du modèle\n",
        "input_dim = 768  # Ajustez selon vos données\n",
        "hidden_dim = 384\n",
        "output_dim = 64\n",
        "modelGNN = GCN3(input_dim, hidden_dim, output_dim)\n",
        "modelGNN.load_state_dict(torch.load('/content/drive/MyDrive/gcn_model.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu-i8-c4QYP6"
      },
      "outputs": [],
      "source": [
        "loader_test = DataLoader(pytorch_geometric_graphs, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOtz-GGwRNw9"
      },
      "outputs": [],
      "source": [
        "train_graphs, test_graphs = train_test_split(pytorch_geometric_graphs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Créez les loaders d'entraînement et de test\n",
        "train_loader = DataLoader(train_graphs, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_graphs, batch_size=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZNCXHPeU2srd"
      },
      "outputs": [],
      "source": [
        "def get_all_embeddings(model, loaders):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_names = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Itérer sur chaque DataLoader fourni\n",
        "    for loader in loaders:\n",
        "        for batch in loader:\n",
        "            with torch.no_grad():\n",
        "                # Obtenir les embeddings du modèle pour le batch actuel\n",
        "                embeddings = model.forward(batch)\n",
        "\n",
        "            # Stocker les embeddings\n",
        "            all_embeddings.extend(embeddings.cpu().numpy())\n",
        "\n",
        "            # Si les noms sont stockés dans le DataLoader, les récupérer\n",
        "           #if hasattr(batch, 'name'):\n",
        "                #all_names.extend(batch.name)\n",
        "            if hasattr(batch, 'name'):\n",
        "               names = [name.split('.')[0] for name in batch.name]  # Split and get first part\n",
        "               all_names.extend(names)\n",
        "\n",
        "\n",
        "            if hasattr(batch, 'y'):\n",
        "              all_labels.extend(batch.y)\n",
        "\n",
        "    return all_embeddings, all_names, all_labels\n",
        "\n",
        "# # Utilisation de la fonction avec plusieurs DataLoader\n",
        "# loaders = [train_loader, test_loader]\n",
        "# loader_test = DataLoader(pytorch_geometric_graphs, batch_size=2, shuffle=True)\n",
        "# embeddings, names , labels = get_all_embeddings(modelGNN, loader_test)\n",
        "\n",
        "# Afficher les résultats pour vérifier\n",
        "#for name, embedding , label  in zip(names, embeddings,labels):\n",
        "    #print(f\"Nom du Graphe: {name}, Label {label}, Embedding: {embedding}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_embeddings(model, batchs):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_names = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Itérer sur chaque DataLoader fourni\n",
        "\n",
        "    for batch in batchs:\n",
        "            with torch.no_grad():\n",
        "                # Obtenir les embeddings du modèle pour le batch actuel\n",
        "                embeddings = model.forward(batch)\n",
        "\n",
        "            # Stocker les embeddings\n",
        "            all_embeddings.extend(embeddings.cpu().numpy())\n",
        "\n",
        "            # Si les noms sont stockés dans le DataLoader, les récupérer\n",
        "           #if hasattr(batch, 'name'):\n",
        "                #all_names.extend(batch.name)\n",
        "            if hasattr(batch, 'name'):\n",
        "               names = [name.split('.')[0] for name in batch.name]  # Split and get first part\n",
        "               all_names.extend(names)\n",
        "\n",
        "\n",
        "            if hasattr(batch, 'y'):\n",
        "              all_labels.extend(batch.y)\n",
        "\n",
        "    return all_embeddings, all_names, all_labels\n",
        "\n",
        "# # Utilisation de la fonction avec plusieurs DataLoader\n",
        "# loaders = [train_loader, test_loader]\n",
        "# loader_test = DataLoader(pytorch_geometric_graphs, batch_size=2, shuffle=True)\n",
        "# embeddings, names , labels = get_all_embeddings(modelGNN, loader_test)\n",
        "\n",
        "# Afficher les résultats pour vérifier\n",
        "#for name, embedding , label  in zip(names, embeddings,labels):\n",
        "    #print(f\"Nom du Graphe: {name}, Label {label}, Embedding: {embedding}\")\n"
      ],
      "metadata": {
        "id": "MXC9-k0wybi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjHUc1wMfpSS"
      },
      "outputs": [],
      "source": [
        "loaders = [train_loader, test_loader]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E58xbpPfcake"
      },
      "outputs": [],
      "source": [
        "def sum_embeddings_with_metadata(embeddings, names, labels, metadata):\n",
        "    summed_vectors = []\n",
        "    final_labels = []  # Liste pour stocker les labels correspondants\n",
        "\n",
        "    for name, embedding, label in zip(names, embeddings, labels):\n",
        "        embedding_tensor = torch.tensor(embedding)\n",
        "        ''' if name in metadata:\n",
        "            metadata_vector = metadata[name]'''\n",
        "        if name in metadata:\n",
        "            metadata_vector = torch.tensor(metadata[name]['embedding'])\n",
        "            if metadata_vector.device != embedding_tensor.device:\n",
        "                metadata_vector = metadata_vector.to(embedding_tensor.device)\n",
        "\n",
        "            summed_vector = embedding_tensor + metadata_vector\n",
        "        else:\n",
        "            print(f\"No metadata available for {name}, using embedding as is.\")\n",
        "            summed_vector = embedding_tensor\n",
        "\n",
        "        summed_vectors.append(summed_vector.numpy())\n",
        "        final_labels.append(label)\n",
        "\n",
        "\n",
        "    return summed_vectors, final_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhEKhGoWeiqH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convertir les données en format approprié\n",
        "X = np.array(summed_vectors)  # Les vecteurs sommés\n",
        "y = np.array(final_labels)    # Les labels correspondants\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Créer le classificateur de forêt aléatoire\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Entraîner le classificateur\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Prédire sur l'ensemble de test\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the classifier: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtH3IMJQUdUY"
      },
      "source": [
        "# Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vNVC1pyKl5Fg"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PkBDH9YjYlBa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Définition des fonctions de nettoyage et de prétraitement du texte\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = re.sub(r'\\W+|\\d+', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Fonction pour créer et enregistrer les métadonnées des dépôts\n",
        "def creer_metadata(repo_url, headers):\n",
        "    documents = []\n",
        "    api_url = f\"https://api.github.com/repos/{repo_url}\"\n",
        "    repo_response = requests.get(api_url, headers=headers)\n",
        "\n",
        "    if repo_response.status_code == 200:\n",
        "        repo_data = repo_response.json()\n",
        "        name = repo_data.get('name', '')\n",
        "        description = repo_data.get('description', '')\n",
        "        topics = ', '.join(repo_data.get('topics', []))\n",
        "        readme_url = f\"https://api.github.com/repos/{repo_url}/contents/README.md\"\n",
        "        readme_response = requests.get(readme_url, headers=headers)\n",
        "        titre=preprocess_text(name)\n",
        "        topics=preprocess_text(topics)\n",
        "        description=preprocess_text(description)\n",
        "\n",
        "        readme_text = ''\n",
        "        if readme_response.status_code == 200:\n",
        "            readme_data = readme_response.json()\n",
        "            content_base64 = readme_data.get('content', '')\n",
        "            readme_text = base64.b64decode(content_base64).decode('utf-8')\n",
        "            readme_text= preprocess_text(readme_text)\n",
        "\n",
        "\n",
        "        # Création d'une entrée pour le dépôt avec son contenu nettoyé\n",
        "        document = {\n",
        "            'name': name,\n",
        "            'text':f\"titre : {titre}, description : {description}, topics : {topics}, readme_text : {readme_text}\"\n",
        "        }\n",
        "        documents.append(document)\n",
        "    else:\n",
        "        print(f\"Erreur lors de la récupération des données pour le dépôt {repo_url}: {repo_response.status_code}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Configuration des headers\n",
        "\n",
        "token = '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'\n",
        "headers = {\n",
        "    'Accept': 'application/vnd.github.v3+json',\n",
        "    'Authorization': f'token {token}'\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbovQzFZTQdL"
      },
      "outputs": [],
      "source": [
        "i = 1\n",
        "metadata_dict_malware = {}  # Dictionnaire pour stocker les métadonnées\n",
        "df = pd.read_csv('/content/popularMalware_sans_duplication.csv')\n",
        "\n",
        "token = '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'\n",
        "headers = {\n",
        "    'Accept': 'application/vnd.github.v3+json',\n",
        "    'Authorization': f'token {token}'\n",
        "}\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    repo_url = f\"{row['Author']}/{row['Repo']}\"\n",
        "    print(f\"nom repo {i}: {row['Repo']}\")\n",
        "    i += 1\n",
        "    metadata = creer_metadata(repo_url, headers)\n",
        "    for data in metadata:\n",
        "        metadata_dict_malware[data['name']] = {'text': data['text']}  # Stocker les métadonnées\n",
        "\n",
        "# Vérification des métadonnées stockées\n",
        "print(metadata_dict_malware)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsoKZi95Tdgn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Suppose que metadata_dict est déjà défini\n",
        "with open('metadataMalware.json', 'w') as json_file:\n",
        "    json.dump(metadata_dict_malware, json_file, indent=4, sort_keys=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV3sG94gTeCX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Charger le fichier JSON\n",
        "with open('metadataMalware.json', 'r') as json_file:\n",
        "    metadata_dict_malware = json.load(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onKiZAWAY80v"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import torch\n",
        "\n",
        "# Charger les métadonnées à partir du fichier JSON\n",
        "with open('metadataMalware.json', 'r') as json_file:\n",
        "    metadata_dict = json.load(json_file)\n",
        "\n",
        "# Préparer les données pour Doc2Vec\n",
        "tagged_data = []\n",
        "for name, content in metadata_dict.items():\n",
        "    text = content['text']\n",
        "    tagged_data.append(TaggedDocument(words=text.split(), tags=[name]))\n",
        "\n",
        "# Entraîner le modèle Doc2Vec\n",
        "model_metadata = Doc2Vec(tagged_data, vector_size=64, window=2, min_count=1, workers=4, dm=0)\n",
        "\n",
        "# Générer les embeddings et créer une nouvelle structure\n",
        "embedding_dict = {}\n",
        "for name, content in metadata_dict.items():\n",
        "    text = content['text']\n",
        "    vector = model_metadata.infer_vector(text.split())\n",
        "    tensor_vector = torch.tensor(vector).tolist()  # Conversion du vecteur en liste pour le stockage JSON\n",
        "    embedding_dict[name] = {'embedding': tensor_vector}\n",
        "\n",
        "# Sauvegarder la nouvelle structure dans un fichier JSON lisible\n",
        "with open('metadata_with_embeddingsMalware.json', 'w') as json_file:\n",
        "    json.dump(embedding_dict, json_file, indent=4, sort_keys=True)\n",
        "\n",
        "# Vérification des métadonnées stockées\n",
        "print(embedding_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaFH8gYS0OOo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Charger le fichier JSON\n",
        "with open('metadata.json', 'r') as json_file:\n",
        "    metadataVectors = json.load(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELS"
      ],
      "metadata": {
        "id": "MJm4BFoUmmKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "class GCN3(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN3, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "        self.bn3 = BatchNorm(output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SHFyw5-2of64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "class GCN1(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN1, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, output_dim)\n",
        "        self.bn1 = BatchNorm(output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "class GCN2(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN2, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim,output_dim)\n",
        "        self.bn2 = BatchNorm(output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "class GCN4(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN4, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn3 = BatchNorm(hidden_dim)\n",
        "        self.conv4 = GCNConv(hidden_dim, output_dim)\n",
        "        self.bn4 = BatchNorm(output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GCN5(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN5, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn3 = BatchNorm(hidden_dim)\n",
        "        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn4 = BatchNorm(hidden_dim)\n",
        "        self.conv5 = GCNConv(hidden_dim, output_dim)\n",
        "        self.bn5 = BatchNorm(output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = self.bn5(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ECzpYbLspRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv, global_mean_pool, BatchNorm\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class GraphSAGEWithLayers(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
        "        super(GraphSAGEWithLayers, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Initialize layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        # Input layer\n",
        "        self.convs.append(SAGEConv(input_dim, hidden_dim if num_layers > 1 else output_dim))\n",
        "        self.bns.append(BatchNorm(hidden_dim if num_layers > 1 else output_dim))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Output layer\n",
        "        if num_layers > 1:\n",
        "            self.convs.append(SAGEConv(hidden_dim, output_dim))\n",
        "            self.bns.append(BatchNorm(output_dim))\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through convolutional layers\n",
        "        for i in range(self.num_layers - 1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Final output layer\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        x = self.bns[-1](x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHOZ3k2zts75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GAT1(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):\n",
        "        super(GAT1, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1 = GATConv(input_dim, output_dim, heads=heads, concat=False)\n",
        "        self.bn1 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through the GAT convolutional layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GAT2(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):\n",
        "        super(GAT2, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False)\n",
        "        self.bn2 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through first convolutional layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through second convolutional layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "class GAT3(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):\n",
        "        super(GAT3, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv3 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False)\n",
        "        self.bn3 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through first convolutional layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through second convolutional layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through third convolutional layer\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "class GAT4(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):\n",
        "        super(GAT4, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn3 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv4 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False)\n",
        "        self.bn4 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through first convolutional layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through second convolutional layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through third convolutional layer\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through fourth convolutional layer\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "class GAT5(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):\n",
        "        super(GAT5, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn3 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv4 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
        "        self.bn4 = BatchNorm(hidden_dim * heads)\n",
        "\n",
        "        self.conv5 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False)\n",
        "        self.bn5 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through first convolutional layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through second convolutional layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through third convolutional layer\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through fourth convolutional layer\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through fifth convolutional layer\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = self.bn5(x)\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8PZ32OJ6cDY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm, global_mean_pool\n",
        "\n",
        "class GraphSAGEWithLayers(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
        "        super(GraphSAGEWithLayers, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Initialize layers with explicit names matching the state dict\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim if num_layers > 1 else output_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim if num_layers > 1 else output_dim)\n",
        "\n",
        "        if num_layers > 1:\n",
        "            self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "            self.bn2 = BatchNorm(hidden_dim)\n",
        "\n",
        "        if num_layers > 2:\n",
        "            self.conv3 = SAGEConv(hidden_dim, output_dim)\n",
        "            self.bn3 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through convolutional layers with explicit names\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            x = self.conv2(x, edge_index)\n",
        "            x = self.bn2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 2:\n",
        "            x = self.conv3(x, edge_index)\n",
        "            x = self.bn3(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GraphSAGEWithLayers(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=4):\n",
        "        super(GraphSAGEWithLayers, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Initialize layers with explicit names matching the state dict\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim if num_layers > 1 else output_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim if num_layers > 1 else output_dim)\n",
        "\n",
        "        if num_layers > 1:\n",
        "            self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "            self.bn2 = BatchNorm(hidden_dim)\n",
        "\n",
        "        if num_layers > 2:\n",
        "            self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
        "            self.bn3 = BatchNorm(hidden_dim)\n",
        "\n",
        "        if num_layers > 3:\n",
        "            self.conv4 = SAGEConv(hidden_dim, output_dim)\n",
        "            self.bn4 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through convolutional layers with explicit names\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            x = self.conv2(x, edge_index)\n",
        "            x = self.bn2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 2:\n",
        "            x = self.conv3(x, edge_index)\n",
        "            x = self.bn3(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 3:\n",
        "            x = self.conv4(x, edge_index)\n",
        "            x = self.bn4(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "class GraphSAGEWithLayers(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=5):\n",
        "        super(GraphSAGEWithLayers, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Initialize layers with explicit names matching the state dict\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim if num_layers > 1 else output_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim if num_layers > 1 else output_dim)\n",
        "\n",
        "        if num_layers > 1:\n",
        "            self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "            self.bn2 = BatchNorm(hidden_dim)\n",
        "\n",
        "        if num_layers > 2:\n",
        "            self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
        "            self.bn3 = BatchNorm(hidden_dim)\n",
        "\n",
        "        if num_layers > 3:\n",
        "            self.conv4 = SAGEConv(hidden_dim, hidden_dim)\n",
        "            self.bn4 = BatchNorm(hidden_dim)\n",
        "\n",
        "        if num_layers > 4:\n",
        "            self.conv5 = SAGEConv(hidden_dim, output_dim)\n",
        "            self.bn5 = BatchNorm(output_dim)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through convolutional layers with explicit names\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            x = self.conv2(x, edge_index)\n",
        "            x = self.bn2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 2:\n",
        "            x = self.conv3(x, edge_index)\n",
        "            x = self.bn3(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 3:\n",
        "            x = self.conv4(x, edge_index)\n",
        "            x = self.bn4(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if self.num_layers > 4:\n",
        "            x = self.conv5(x, edge_index)\n",
        "            x = self.bn5(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "0qYw929Wjz4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train classifier"
      ],
      "metadata": {
        "id": "CyVoJ4A_Nmvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aPGsSjJA0oYs"
      },
      "outputs": [],
      "source": [
        "embeddings, names, labels = get_all_embeddings(modelGNN, loaders)\n",
        "\n",
        "summed_vectors, final_labels = sum_embeddings_with_metadata(embeddings, names, labels, metadataVectors)\n",
        "\n",
        "# Afficher les résultats pour vérifier\n",
        "for name, summed_vector, label in zip(names, summed_vectors, final_labels):\n",
        "    print(f\"Nom du Graphe: {name}, Label: {label}, Vecteur Sommé: {summed_vector}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeF7rUEECTl1"
      },
      "outputs": [],
      "source": [
        "embeddings, names, labels = get_all_embeddings(modelGNN, loaders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7zhGdchTgYn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualisation avec t-SNE\n",
        "def plot_embeddings(X, y, title):\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for label in np.unique(y):\n",
        "        indices = np.where(y == label)\n",
        "        plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=f\"Class {label}\")\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"t-SNE component 1\")\n",
        "    plt.ylabel(\"t-SNE component 2\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting the training set\n",
        "plot_embeddings(X_train, y_train, \"Training set\")\n",
        "\n",
        "# Plotting the test set with predicted labels\n",
        "plot_embeddings(X_test, y_pred, \"Test set with predicted labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPajkNOnEEMM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming get_all_embeddings and sum_embeddings_with_metadata functions are defined elsewhere\n",
        "\n",
        "# Get embeddings, names, and labels\n",
        "embeddings, names, labels = get_all_embeddings(modelGNN, loaders)\n",
        "summed_vectors, final_labels = sum_embeddings_with_metadata(embeddings, names, labels, metadataVectors)\n",
        "\n",
        "# Convert data to NumPy arrays\n",
        "X = np.array(summed_vectors)  # The summed vectors\n",
        "y = np.array(final_labels)    # The corresponding labels\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Random Forest classifier with 100 trees\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Random Forest classifier: {accuracy:.2f}\")\n",
        "\n",
        "# You can further use the trained model (rf_clf) to predict labels for new data points\n",
        "# by calling rf_clf.predict(new_summed_vector)\n",
        "\n",
        "with open('graphe_appel_classifier.pkl', 'wb') as model_file:\n",
        "    pickle.dump(rf_clf, model_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhR4fkx77pRE"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Réduire la dimensionnalité avec t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_embedded = tsne.fit_transform(X)\n",
        "\n",
        "# Afficher le graphique de classification\n",
        "plt.figure(figsize=(10, 8))\n",
        "for label in np.unique(y):\n",
        "    plt.scatter(X_embedded[y == label, 0], X_embedded[y == label, 1], label=label)\n",
        "plt.title('t-SNE Projection of Embeddings')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "9D0AuRT5oj-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "8KTzGs9OBktD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.3.2\n",
        "import pickle\n",
        "with open('graphe_appel_classifier.pkl', 'rb') as f:\n",
        "  rf_clf_loaded = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "buU1PgJut0lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_embeddings(model, batchs):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_names = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Itérer sur chaque DataLoader fourni\n",
        "\n",
        "    for batch in batchs:\n",
        "            with torch.no_grad():\n",
        "                # Obtenir les embeddings du modèle pour le batch actuel\n",
        "                embeddings = model.forward(batch)\n",
        "\n",
        "            # Stocker les embeddings\n",
        "            all_embeddings.extend(embeddings.cpu().numpy())\n",
        "\n",
        "            # Si les noms sont stockés dans le DataLoader, les récupérer\n",
        "           #if hasattr(batch, 'name'):\n",
        "                #all_names.extend(batch.name)\n",
        "            if hasattr(batch, 'name'):\n",
        "               names = [name.split('.')[0] for name in batch.name]  # Split and get first part\n",
        "               all_names.extend(names)\n",
        "\n",
        "\n",
        "            if hasattr(batch, 'y'):\n",
        "              all_labels.extend(batch.y)\n",
        "\n",
        "    return all_embeddings, all_names, all_labels\n",
        "\n",
        "# # Utilisation de la fonction avec plusieurs DataLoader\n",
        "# loaders = [train_loader, test_loader]\n",
        "# loader_test = DataLoader(pytorch_geometric_graphs, batch_size=2, shuffle=True)\n",
        "# embeddings, names , labels = get_all_embeddings(modelGNN, loader_test)\n",
        "\n",
        "# Afficher les résultats pour vérifier\n",
        "#for name, embedding , label  in zip(names, embeddings,labels):\n",
        "    #print(f\"Nom du Graphe: {name}, Label {label}, Embedding: {embedding}\")\n"
      ],
      "metadata": {
        "id": "Xh_BX_hA1xFU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "# Charger les données malveillantes\n",
        "with open('/content/drive/MyDrive/Dataset/Test/windows/windows_malware.pkl', 'rb') as f:\n",
        "    loader_malware = pickle.load(f)\n",
        "\n",
        "# Charger les données bénignes\n",
        "with open('/content/drive/MyDrive/Dataset/Test/windows/windows_benin.pkl', 'rb') as f:\n",
        "    loader_benign = pickle.load(f)\n",
        "# Convertir les données malveillantes en objets PyTorch Geometric\n",
        "pytorch_geometric_graphs_malware = [\n",
        "    Data(x=entry['data'].x, edge_index=entry['data'].edge_index, y=torch.tensor([entry['label']]), name=entry['name'])\n",
        "    for entry in loader_malware\n",
        "]\n",
        "\n",
        "# Convertir les données bénignes en objets PyTorch Geometric\n",
        "pytorch_geometric_graphs_benign = [\n",
        "    Data(x=entry['data'].x, edge_index=entry['data'].edge_index, y=torch.tensor([entry['label']]), name=entry['name'])\n",
        "    for entry in loader_benign\n",
        "]\n",
        "# Combiner les deux ensembles de données (malware et bénin)\n",
        "pytorch_geometric_graphs_combined = pytorch_geometric_graphs_malware + pytorch_geometric_graphs_benign\n",
        "# Créer un DataLoader avec les données combinées\n",
        "myloader = DataLoader(pytorch_geometric_graphs_combined, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0QfUSUPwK9",
        "outputId": "f37842ce-2796-4a79-99d2-ab976c3a9ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 768  # Dimension d'entrée\n",
        "hidden_dim = 384  # Dimension cachée (facultatif, non utilisé ici)\n",
        "output_dim = 64  # Dimension de sortie\n",
        "heads = 8# Nombre de têtes d'attention\n",
        "\n",
        "# Instanciation du modèle\n",
        "modelGNN = GCN3(input_dim, hidden_dim, output_dim)\n",
        "modelGNN.load_state_dict(torch.load('/content/drive/MyDrive/gcn_model.pth'))\n",
        "\n",
        "print(modelGNN)"
      ],
      "metadata": {
        "id": "byA6eAJfa33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Charger les données malveillantes\n",
        "with open('/content/drive/MyDrive/Dataset/Test/windows/windows_malware.pkl', 'rb') as f:\n",
        "    loader_malware = pickle.load(f)\n",
        "\n",
        "# Charger les données bénignes\n",
        "with open('/content/drive/MyDrive/Dataset/Test/windows/windows_benin.pkl', 'rb') as f:\n",
        "    loader_benign = pickle.load(f)\n",
        "\n",
        "# Combiner les deux ensembles de données (malware et bénin)\n",
        "combined_data = loader_malware + loader_benign\n",
        "\n",
        "# Sauvegarder les données combinées\n",
        "combined_data_path = '/content/windows.pkl'\n",
        "with open(combined_data_path, 'wb') as f:\n",
        "    pickle.dump(combined_data, f)\n",
        "\n",
        "print(f\"Les données combinées ont été sauvegardées dans {combined_data_path}\")\n"
      ],
      "metadata": {
        "id": "cXvgNtZY28hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "\n",
        "#with open('/content/drive/MyDrive/Dataset/Test/virus/virus.pkl', 'rb') as f:\n",
        "with open('/content/windows.pkl', 'rb') as f:\n",
        "   loader = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "pytorch_geometric_graphs = [\n",
        "    Data(x=entry['data'].x, edge_index=entry['data'].edge_index, y=torch.tensor([entry['label']]) , name=entry['name'])\n",
        "    for entry in loader\n",
        "]\n",
        "\n",
        "myloader = DataLoader(pytorch_geometric_graphs, batch_size=2, shuffle=True)\n",
        "embeddings, names, labels = get_all_embeddings(modelGNN,myloader)"
      ],
      "metadata": {
        "id": "8RHAl8lVwLXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Charger les fichiers JSON\n",
        "with open('/content/drive/MyDrive/Dataset/Test/windows/metadata_windows_benin.json', 'r') as f1, open('/content/drive/MyDrive/Dataset/Test/android/metadata_android_malware.json', 'r') as f2:\n",
        "    data1 = json.load(f1)\n",
        "    data2 = json.load(f2)\n",
        "\n",
        "# Vérifier les structures\n",
        "print(f\"Structure de data1: {type(data1)}\")\n",
        "print(f\"Structure de data2: {type(data2)}\")\n",
        "\n",
        "# Fusionner en fonction des structures\n",
        "if isinstance(data1, dict) and isinstance(data2, dict):\n",
        "    data_merged = {**data1, **data2}  # Fusionner les dictionnaires\n",
        "elif isinstance(data1, list) and isinstance(data2, list):\n",
        "    data_merged = data1 + data2  # Fusionner les listes\n",
        "else:\n",
        "    raise ValueError(\"Les structures des fichiers JSON ne sont pas compatibles pour la fusion.\")\n",
        "\n",
        "# Sauvegarder les données fusionnées dans un nouveau fichier JSON\n",
        "with open('file.json', 'w') as f_out:\n",
        "    json.dump(data_merged, f_out, indent=4)\n"
      ],
      "metadata": {
        "id": "q5yVjcrrnAcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_embeddings_with_metadata(embeddings, names, labels, metadata):\n",
        "    summed_vectors = []\n",
        "    final_labels = []  # Liste pour stocker les labels correspondants\n",
        "\n",
        "    for name, embedding, label in zip(names, embeddings, labels):\n",
        "        embedding_tensor = torch.tensor(embedding)\n",
        "        ''' if name in metadata:\n",
        "            metadata_vector = metadata[name]'''\n",
        "        if name in metadata:\n",
        "            metadata_vector = torch.tensor(metadata[name]['embedding'])\n",
        "            if metadata_vector.device != embedding_tensor.device:\n",
        "                metadata_vector = metadata_vector.to(embedding_tensor.device)\n",
        "\n",
        "            summed_vector = embedding_tensor + metadata_vector\n",
        "        else:\n",
        "            print(f\"No metadata available for {name}, using embedding as is.\")\n",
        "            summed_vector = embedding_tensor\n",
        "\n",
        "        summed_vectors.append(summed_vector.numpy())\n",
        "        final_labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return summed_vectors, final_labels\n"
      ],
      "metadata": {
        "id": "oFbLc3vYn6bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Assuming get_all_embeddings and sum_embeddings_with_metadata functions are defined elsewhere\n",
        "\n",
        "# Get embeddings, names, and labels\n",
        "\n",
        "\n",
        "embeddings, names, labels = get_all_embeddings(modelGNN,myloader)\n",
        "summed_vectors, final_labels = sum_embeddings_with_metadata(embeddings, names, labels, data_merged)\n",
        "\n",
        "\n",
        "# Faire des prédictions sur le nouveau dataset de test\n",
        "y_pred_new = rf_clf_loaded.predict(summed_vectors)\n",
        "\n",
        "# Calculer les métriques de performance\n",
        "accuracy = accuracy_score(final_labels, y_pred_new)\n",
        "precision = precision_score(final_labels, y_pred_new, average='binary')\n",
        "recall = recall_score(final_labels, y_pred_new, average='binary')\n",
        "f1 = f1_score(final_labels, y_pred_new, average='binary')\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9iydyGcJmpYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('notre_android.pkl', 'wb') as f:\n",
        "    # Utilisez pickle.dump pour écrire les objets dans le fichier\n",
        "    pickle.dump((summed_vectors, final_labels), f)"
      ],
      "metadata": {
        "id": "doVve1uhwd_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "# Charger les données malveillantes\n",
        "with open('/content/drive/MyDrive/Dataset/Test/virus/virus_malware.pkl', 'rb') as f:\n",
        "    loader_malware = pickle.load(f)\n",
        "\n",
        "# Charger les données bénignes\n",
        "with open('/content/drive/MyDrive/Dataset/Test/virus/virus_benin.pkl', 'rb') as f:\n",
        "    loader_benign = pickle.load(f)\n",
        "# Convertir les données malveillantes en objets PyTorch Geometric\n",
        "pytorch_geometric_graphs_malware = [\n",
        "    Data(x=entry['data'].x, edge_index=entry['data'].edge_index, y=torch.tensor([entry['label']]), name=entry['name'])\n",
        "    for entry in loader_malware\n",
        "]\n",
        "\n",
        "# Convertir les données bénignes en objets PyTorch Geometric\n",
        "pytorch_geometric_graphs_benign = [\n",
        "    Data(x=entry['data'].x, edge_index=entry['data'].edge_index, y=torch.tensor([entry['label']]), name=entry['name'])\n",
        "    for entry in loader_benign\n",
        "]\n",
        "# Combiner les deux ensembles de données (malware et bénin)\n",
        "pytorch_geometric_graphs_combined = pytorch_geometric_graphs_malware + pytorch_geometric_graphs_benign\n",
        "# Créer un DataLoader avec les données combinées\n",
        "myloader = DataLoader(pytorch_geometric_graphs_combined, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "yl6A8kiBpZyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Supposons que get_all_embeddings et sum_embeddings_with_metadata sont définis ailleurs\n",
        "\n",
        "# Obtenez les embeddings, les noms et les labels\n",
        "embeddings, names, labels = get_all_embeddings(modelGNN, myloader)\n",
        "summed_vectors, final_labels = sum_embeddings_with_metadata(embeddings, names, labels, data_merged)\n",
        "\n",
        "# Convertir les vecteurs en tableaux NumPy\n",
        "summed_vectors = np.array(summed_vectors)\n",
        "final_labels = np.array(final_labels)\n",
        "\n",
        "# Prédictions sur le dataset de test\n",
        "y_pred_new = rf_clf_loaded.predict(summed_vectors)\n",
        "\n",
        "# Calculer les métriques de performance\n",
        "accuracy = accuracy_score(final_labels, y_pred_new)\n",
        "precision = precision_score(final_labels, y_pred_new, average='binary')\n",
        "recall = recall_score(final_labels, y_pred_new, average='binary')\n",
        "f1 = f1_score(final_labels, y_pred_new, average='binary')\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Rapport de classification\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(final_labels, y_pred_new, target_names=['benign', 'malware']))\n",
        "\n",
        "# Visualisation avec t-SNE\n",
        "def plot_embeddings(X, y, title):\n",
        "    tsne = TSNE(n_components=2, perplexity=min(30, len(X)-1), random_state=42)  # Ajuster perplexity\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='coolwarm', s=50, alpha=0.7, edgecolor='k')\n",
        "    plt.colorbar(scatter, label='Class')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"t-SNE component 1\")\n",
        "    plt.ylabel(\"t-SNE component 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Assurez-vous que les labels sont sous forme numérique pour la visualisation\n",
        "if np.issubdtype(final_labels.dtype, np.number):\n",
        "    numeric_labels = final_labels\n",
        "    numeric_pred = y_pred_new\n",
        "else:\n",
        "    # Convertir les labels en valeurs numériques\n",
        "    label_mapping = {'benign': 0, 'malware': 1}\n",
        "    numeric_labels = np.array([label_mapping[label] for label in final_labels])\n",
        "    numeric_pred = np.array([label_mapping[label] for label in y_pred_new])\n",
        "\n",
        "# Afficher les embeddings avec les labels prédits\n",
        "plot_embeddings(summed_vectors, numeric_pred, \"t-SNE Visualization of Test Set with Predicted Labels\")"
      ],
      "metadata": {
        "id": "ZusJhsB6vGZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n"
      ],
      "metadata": {
        "id": "-p9w_1AtC5nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install streamlit-option-menu\n",
        "!pip install streamlit-lottie"
      ],
      "metadata": {
        "id": "pqKJLDj5chWB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation de la bibliotheque pycg qui genre les graphe d'appel\n",
        "!git clone https://github.com/vitsalis/PyCG.git\n",
        "%cd PyCG\n",
        "!pip install .\n"
      ],
      "metadata": {
        "id": "_3i5ZtKdOoj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "0_v2hGT_OpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2K5Fn0LG8Miigzi0zMCW2oCoyKd_797ecEbvtBoxk5HCzCcP3"
      ],
      "metadata": {
        "id": "u8AeRc7GE1Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install scikit-learn==1.3.2\n"
      ],
      "metadata": {
        "id": "RCKVyJ44NG-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Charger les stopwords anglais\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "EqfLBBr0OR6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate ngrok (if not already done)\n",
        "ngrok.set_auth_token(\"2K5Fn0LG8Miigzi0zMCW2oCoyKd_797ecEbvtBoxk5HCzCcP3\")\n",
        "\n",
        "# Start ngrok tunnel to streamlit port\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit is available at: {public_url}\")\n"
      ],
      "metadata": {
        "id": "40VkwurQC-Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3V68rAWApfBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "tunnels = ngrok.get_tunnels()\n",
        "print(\"Active tunnels:\", tunnels)"
      ],
      "metadata": {
        "id": "1Mp91x0Iorx1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y67NlcnStF8j",
        "S7nTDklsTCIF",
        "vksZ16A7bCQw",
        "xCYJS2uAa5Z9",
        "4VM2q48-G8Sr",
        "MJm4BFoUmmKF",
        "CyVoJ4A_Nmvi",
        "9D0AuRT5oj-A"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}